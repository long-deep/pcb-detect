{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cTransGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmTXfm5o5rdj"
      },
      "source": [
        "#TransGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4cDcGO05qxw"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykE2UP6C5zDc"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, D, heads=8):\n",
        "        super().__init__()\n",
        "        self.D = D\n",
        "        self.heads = heads\n",
        "\n",
        "        assert (D % heads == 0), \"Embedding size should be divisble by number of heads\"\n",
        "        self.head_dim = self.D // heads\n",
        "\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.H = nn.Linear(self.D, self.D)\n",
        "\n",
        "    def forward(self, Q, K, V, mask):\n",
        "        batch_size = Q.shape[0]\n",
        "        q_len, k_len, v_len = Q.shape[1], K.shape[1], V.shape[1]\n",
        "\n",
        "        Q = Q.reshape(batch_size, q_len, self.heads, self.head_dim)\n",
        "        K = K.reshape(batch_size, k_len, self.heads, self.head_dim)\n",
        "        V = V.reshape(batch_size, v_len, self.heads, self.head_dim)\n",
        "\n",
        "        # performing batch-wise matrix multiplication\n",
        "        raw_scores = torch.einsum(\"bqhd,bkhd->bhqk\", [Q, K])\n",
        "\n",
        "        # shut off triangular matrix with very small value\n",
        "        scores = raw_scores.masked_fill(mask == 0, -np.inf) if mask else raw_scores\n",
        "\n",
        "        attn = torch.softmax(scores / np.sqrt(self.D), dim=3)\n",
        "        attn_output = torch.einsum(\"bhql,blhd->bqhd\", [attn, V])\n",
        "        attn_output = attn_output.reshape(batch_size, q_len, self.D)\n",
        "\n",
        "        output = self.H(attn_output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMWJccpU53UF"
      },
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, D, heads, p, fwd_exp):\n",
        "        super().__init__()\n",
        "        self.mha = Attention(D, heads)\n",
        "        self.drop_prob = p\n",
        "        self.n1 = nn.LayerNorm(D)\n",
        "        self.n2 = nn.LayerNorm(D)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(D, fwd_exp*D),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fwd_exp*D, D),\n",
        "        )\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, Q, K, V, mask):\n",
        "        attn = self.mha(Q, K, V, mask)\n",
        "\n",
        "        \"\"\"\n",
        "        Layer normalisation with residual connections\n",
        "        \"\"\"\n",
        "        x = self.n1(attn + Q)\n",
        "        x = self.dropout(x)\n",
        "        forward = self.mlp(x)\n",
        "        x = self.n2(forward + x)\n",
        "        out = self.dropout(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, noise_w, noise_h, channels):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(\n",
        "                    noise_w*noise_h*channels, \n",
        "                    (8*8)*noise_w*noise_h*channels, \n",
        "                    bias=False\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        return out\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self, emb_w, emb_h, channels):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(\n",
        "                    emb_w*emb_h*channels, \n",
        "                    (8*8)*emb_w*emb_h*channels, \n",
        "                    bias=False\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        return out\n",
        "\n",
        "class PixelShuffle(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        pass\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mlp = MLP(32, 32, 1)\n",
        "        self.emb = Embedding(384, 1, 1)\n",
        "        \n",
        "        # stage 1\n",
        "        self.s1_enc = nn.ModuleList([\n",
        "                        EncoderBlock(1024*8*8)\n",
        "                        for _ in range(5)\n",
        "                    ])\n",
        "\n",
        "        # stage 2\n",
        "        self.s2_pix_shuffle = PixelShuffle()\n",
        "        self.s2_enc = nn.ModuleList([\n",
        "                        EncoderBlock(256*16*16)\n",
        "                        for _ in range (4)\n",
        "                    ])\n",
        "\n",
        "        # stage 3\n",
        "        self.s3_pix_shuffle = PixelShuffle()\n",
        "        self.s3_enc = nn.ModuleList([\n",
        "                        EncoderBlock(64*32*32)\n",
        "                        for _ in range(2)\n",
        "                    ])\n",
        "\n",
        "        # stage 4\n",
        "        self.linear = nn.Linear(32*32*64, 32*32*3)\n",
        "\n",
        "    def forward(self, noise, embedding):\n",
        "        x = self.mlp(noise)\n",
        "        embedding = self.emb(embedding)\n",
        "        x = torch.cat([x,embedding],1)\n",
        "        for layer in self.s1_enc:\n",
        "            x = layer(x)\n",
        "        \n",
        "        x = self.s2_pix_shuffle(x)\n",
        "        for layer in self.s2_enc:\n",
        "            x = layer(x)\n",
        "\n",
        "        x - self.s3_pix_shuffle(x)\n",
        "        for layer in self.s3_enc:\n",
        "            x = layer(x)\n",
        "\n",
        "        text = self.linear(x)\n",
        "\n",
        "        return text\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, max_len_word,out_num):\n",
        "        super().__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(max_len_word*256, (8*8+1)*384)\n",
        "        self.s2_enc = nn.ModuleList([\n",
        "                        EncoderBlock((8*8+1)*284)\n",
        "                        for _ in range(7)\n",
        "                    ])\n",
        "\n",
        "        self.classification_head = nn.Linear(1*384, out_num)\n",
        "\n",
        "    def forward(self, text):\n",
        "        x = self.l1(text)\n",
        "        for layer in self.s2_enc:\n",
        "            x = layer(x)\n",
        "\n",
        "        logits = self.classification_head(x)\n",
        "        pred = F.softmax(logits)\n",
        "        embedding = x\n",
        "        return pred,embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding训练\n",
        "def train_classifier(trainloader, D, D_optimizer, loss_func, device):\n",
        "\n",
        "    # set train mode\n",
        "    D.train()\n",
        "    \n",
        "    D_total_loss = 0\n",
        "    \n",
        "    \n",
        "    for i, x in enumerate(trainloader):\n",
        "        # real label and fake label\n",
        "        y_real = torch.ones(x.size(0), 1).to(device).float()\n",
        "        # batch_size个真实数据\n",
        "        x = x.to(device)\n",
        "\n",
        "        # update D network\n",
        "        # D optimizer zero grads\n",
        "        D_optimizer.zero_grad()\n",
        "        \n",
        "        # D real loss from real images\n",
        "        d_real = D(x)[0]\n",
        "        d_real_loss = loss_func(d_real.float(), y_real)\n",
        "        d_real_loss.backward()\n",
        "        D_optimizer.step()\n",
        "\n",
        "        D_total_loss += d_loss.item()\n",
        "    \n",
        "    return D_total_loss / len(trainloader)"
      ],
      "metadata": {
        "id": "uZ880LLileYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding 训练\n",
        "max_len_word = 32\n",
        "\n",
        "# Adam lr and betas\n",
        "learning_rate = 1e-4\n",
        "betas = (0.5, 0.999)\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "device = torch.device('cpu')\n",
        "train_value = Variable(torch.from_numpy('''训练数据'''))\n",
        "trainloader = torch.utils.data.DataLoader(train_value, batch_size=batch_size, shuffle=True)\n",
        "bceloss = nn.BCELoss().to(device).double()\n",
        "\n",
        "D = Discriminator(max_len_word,2).to(device).double()\n",
        "# G and D optimizer, use Adam or SGD\n",
        "D_optimizer = optim.Adam(D.parameters(), lr=learning_rate, betas=betas)\n",
        "\n",
        "n_epochs = 1800\n",
        "\n",
        "d_loss_hist = []\n",
        "train_classifier(trainloader, D, D_optimizer, loss_func, device)"
      ],
      "metadata": {
        "id": "kViidIZ_vtuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VhEGK25EwgA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDabzrMV6Dmf"
      },
      "source": [
        "# GAN训练\n",
        "def train(trainloader, G, D, G_optimizer, D_optimizer, loss_func, device):\n",
        "\n",
        "    # set train mode\n",
        "    D.train()\n",
        "    G.train()\n",
        "    \n",
        "    D_total_loss = 0\n",
        "    G_total_loss = 0\n",
        "    \n",
        "    \n",
        "    for i, x in enumerate(trainloader):\n",
        "        # real label and fake label\n",
        "        y_real = torch.ones(x.size(0), 1).to(device).float()\n",
        "        y_fake = torch.zeros(x.size(0), 1).to(device).float()\n",
        "        # batch_size个真实数据\n",
        "        x = x.to(device)\n",
        "\n",
        "        # update D network\n",
        "        # D optimizer zero grads\n",
        "        D_optimizer.zero_grad()\n",
        "        \n",
        "        # D real loss from real images\n",
        "        d_real = D(x)[0]\n",
        "        d_real_loss = loss_func(d_real.float(), y_real)\n",
        "        \n",
        "        # D fake loss from fake images generated by G\n",
        "        g_z = G()\n",
        "        d_fake = D(g_z)[0]\n",
        "        d_fake_loss = loss_func(d_fake.float(), y_fake)\n",
        "        \n",
        "        # D backward and step\n",
        "        d_loss = d_real_loss + d_fake_loss\n",
        "        d_loss.backward()\n",
        "        D_optimizer.step()\n",
        "\n",
        "        # update G network\n",
        "        # G optimizer zero grads\n",
        "        G_optimizer.zero_grad()\n",
        "        \n",
        "        # G loss\n",
        "        g_z = G()\n",
        "        d_fake = D(g_z)[0]\n",
        "        g_loss = loss_func(d_fake.float(), y_real)\n",
        "        \n",
        "        # G backward and step\n",
        "        g_loss.backward()\n",
        "        G_optimizer.step()\n",
        "        \n",
        "        D_total_loss += d_loss.item()\n",
        "        G_total_loss += g_loss.item()\n",
        "    \n",
        "    return D_total_loss / len(trainloader), G_total_loss / len(trainloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtI_UUm39J5-"
      },
      "source": [
        "# cTransGAN 训练\n",
        "max_len_word = 32\n",
        "\n",
        "# Adam lr and betas\n",
        "learning_rate = 1e-4\n",
        "betas = (0.5, 0.999)\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "device = torch.device('cpu')\n",
        "train_value = Variable(torch.from_numpy('''训练数据'''))\n",
        "trainloader = torch.utils.data.DataLoader(train_value, batch_size=batch_size, shuffle=True)\n",
        "bceloss = nn.BCELoss().to(device).double()\n",
        "\n",
        "# G and D model\n",
        "G = Generator().to(device).double()\n",
        "D = Discriminator(max_len_word,2).to(device).double()\n",
        "# G and D optimizer, use Adam or SGD\n",
        "G_optimizer = optim.Adam(G.parameters(), lr=learning_rate, betas=betas)\n",
        "D_optimizer = optim.Adam(D.parameters(), lr=learning_rate, betas=betas)\n",
        "\n",
        "n_epochs = 1800\n",
        "\n",
        "d_loss_hist = []\n",
        "g_loss_hist = []\n",
        "train(trainloader, G, D, G_optimizer, D_optimizer, loss_func, device, z_dim)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}